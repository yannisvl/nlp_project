{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1η Εργαστηριακή Άσκηση : Μέρος Πρώτο , Ορθογράφος\n",
    "\n",
    "## Επεξεργασία φωνής και φυσικής γλώσσας\n",
    "\n",
    "### Βλάχος Ιωάννης, 03115013\n",
    "### ΣΗΜΜΥ, 9ο εξάμηνο, ροή Σ\n",
    "### Ακαδημαϊκό Έτος 2019-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1 : κατασκευή corpus\n",
    "α) Αρχικά παίρνω τα 2 προτεινόμενα βιβλία και τα κάνω ένα αρχείο merged.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"1661-0.txt\"\n",
    "p2 = \"36-0.txt\"\n",
    "filenames = [p1, p2]\n",
    "with open('merged.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "p = \"merged.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Εκτός από τον μεγαλύτερο όγκο δεδομένων, υπάρχει και μεγαλύτερη ποικιλία στη θεματολογία των λέξεων που χρησιμοποιούνται. Για παράδειγμα, δε θέλω να εμφανίζεται πολλές φορές η χρήση της λέξης \"φόνος\" αν έχω επιλέξει μόνο ένα αστυνομικό μυθιστόρημα. Εκτός από αυτό, υπάρχει και ποικιλία στον λόγο γενικότερα αν επιλεχθούν περισσότερα από ένα βιβλία. Συμπεριλαμβάνονται έτσι και επίσημοι και πιο καθημερινοί διάλογοι, ενώ οι εκφράσεις και ο τρόπος γραφής 2 διαφορετικών συγγραφέων προσδίδει καλύτερη δειγματοληψία φυσικής γλώσσας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2 : προπαρασκευή corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_process(s):\n",
    "    return s\n",
    "\n",
    "\n",
    "def readFile(pathname, preprocess=identity_process):\n",
    "    f = open(pathname, \"r\")\n",
    "    mylist = []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        mylist.extend(preprocess(line))\n",
    "    return mylist\n",
    "\n",
    "        \n",
    "def tokenize(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    news=\"\"\n",
    "    for a in s:\n",
    "        if a==' ' or a=='\\n': news+=' '\n",
    "        elif a.isalpha(): news+=a\n",
    "    return news.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'test', 'what', 'it', 'does', 'to', '@', '!', 'Symbols', 'and', '23numbers', '!', '?', '?', ':', 'as', 'well', ',', ',', ',', 'a/', '.', 'as', 'otheRS']\n",
      "['lets', 'test', 'what', 'it', 'does', 'to', 'symbols', 'and', 'numbers', 'as', 'wella', 'as', 'others']\n"
     ]
    }
   ],
   "source": [
    "text = \"Let's test what it does to @!Symbols and 23numbers! ??:as well,,,a/. as otheRS\"\n",
    "\n",
    "a = nltk.word_tokenize(text)\n",
    "print(a)\n",
    "print(tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα σημεία στίξης τα τυπώνει ξεχωριστά εκτός από την απόστροφο που την κολλάει στις γειτονικές λέξεις. Διαφορετικά κάνει split ανάλογα με τα κενά. Ο ζητούμενος tokenizer που υλοποιήθηκε παραπάνω κρατάει μόνο τους χαρακτήρες και δίνει ως αποτέλεσμα λέξεις μικρών λατινικών χαρακτήρων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3 : Κατασκευή λεξικού και αλφαβήτου\n",
    "\n",
    "Με τη βοήθεια της μεθόδου που φτιάχτηκε παραπάνω διαβάζουμε το κείμενο με παράμετρο την tokenize και μετατρέποντας το set σε λίστα, κρατάμε όλες τις διαφορετικές λέξεις που υπάρχουν στο corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = readFile(p,tokenize)\n",
    "tokens = list(set(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['observerexcellent',\n",
       " 'for',\n",
       " 'drawing',\n",
       " 'the',\n",
       " 'veil',\n",
       " 'from',\n",
       " 'mens',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'but',\n",
       " 'for',\n",
       " 'the',\n",
       " 'trained',\n",
       " 'reasoner',\n",
       " 'to',\n",
       " 'admit',\n",
       " 'such',\n",
       " 'intrusions',\n",
       " 'into']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok[300:320]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιώντας το λεξικό λέξεων, παίρνουμε όλα τα γράμματα που υπάρχουν στις λέξεις του λεξικού. \n",
    "Φτιάχνοντας ομοίως ένα set από αυτά κρατάμε όλους τους διαφορετικούς χαρακτήρες του corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'k', 'q', 'ç', 'h', 's', 'w', 'j', 'f', 't', 'u', 'n', 'g', 'i', 'â', 'z', 'v', 'æ', 'l', 'è', 'a', 'é', 'r', 'o', 'œ', 'm', 'c', 'd', 'b', 'y', 'à', 'e', 'x']\n"
     ]
    }
   ],
   "source": [
    "l = [list(i) for i in tokens]\n",
    "flat_list = [item for sublist in l for item in sublist]\n",
    "symbols=list(set(flat_list))\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4\n",
    "\n",
    "Αντιστοιχίζουμε κάθε χαρακτήρα σε ένα αύξοντα index. Κάνουμε το ίδιο και για τις λέξεις σε περίπτωση που χρειαστεί να βγάλουμε output μια ολόκληρη λέξη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOFST(alphabet,s):\n",
    "    f= open(s,\"w+\")\n",
    "    f.write(\"<epsilon> 0\\n\")\n",
    "    count=1\n",
    "    for i in alphabet:\n",
    "        if i=='\\n': f.write(\"<newline>\" + '\\t' + str(count) + '\\n')\n",
    "        elif i==' ': f.write(\"<space>\" + '\\t' + str(count) + '\\n')\n",
    "        else: f.write(i + '\\t' + str(count) + '\\n')\n",
    "        count+=1\n",
    "    f.close()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='out.syms' mode='w+' encoding='UTF-8'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeOFST(symbols, \"chars.syms\")\n",
    "makeOFST(tokens, \"out.syms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text.fst', mode = 'w')\n",
    "\n",
    "def format_arc(src, dst, src_sym, dst_sym, w):\n",
    "    f.write(str(src)+\" \"+str(dst)+\" \"+src_sym+\" \"+dst_sym+\" \"+str(w)+'\\n')\n",
    "    return\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[i], w=0)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=\"<epsilon>\", w=1)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=symbols[i], w=1)\n",
    "    for j in range(len(symbols)):\n",
    "        if j!=i:\n",
    "            format_arc(\n",
    "                src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[j], w=1)\n",
    "f.write('0')\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βάζουμε τη μοναδική κατάσταση ως τελική για να μπορούμε να συνδυάσουμε το αυτόματο με άλλα fst.\n",
    "\n",
    "α) Έχοντας φτιάξει το αρχείο text.fst στο οποίο υπάρχουν όλες οι ζητούμενες μεταβάσεις του αυτόματου, τρέχουμε την εντολή \n",
    "\n",
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols text.fst transducer.fst\n",
    "\n",
    "για να φτιάξουμε τον transducer. \n",
    "Αν σε αυτόν τον transducer εφαρμόσουμε τον αλγόριθμο ShortestPath για μία συγκεκριμένη είσοδο θα μας δώσει την ίδια τη λέξη μαζί με το πλήθος των μεταβάσεων σε διαφορετικά γράμματα. Δηλαδή αν δεν είχε δύο ίδια συνεχόμενα γράμματα θα μας έδινε το μήκος της λέξης -1.\n",
    "\n",
    "β) Ωστόσο η μέθοδος αυτή δε μας δίνει κάποια προτίμηση ως προς το ποια γράμματα αλλάζουμε και με ποια πράξη.\n",
    "Για παράδειγμα το deletion ενός χαρακτήρα μπορεί να κοστίζει περισσότερο αν αυτός ο χαρακτήρας είναι σπάνιος, μια και χαρακτηρίζει περισσότερο τη λέξη εισόδου από ό,τι ένα φωνήεν. Το ίδιο ισχύει και για το insertion.\n",
    "Επομένως σε αυτά τα βάρη θα μπορούσαμε να βάλουμε τιμές αντιστρόφως ανάλογες στη συχνότητα εμφάνισης των γραμμάτων που εισέρχονται ή διαγράφονται.\n",
    "\n",
    "Επίσης, η μετάβαση από ένα γράμμα σε ένα άλλο θα πρέπει επίσης να κοστίζει παραπάνω αν το ζεύγος που προκύπτει είναι σπάνιο, καθώς διαφοροποιεί σημαντικά τη λέξη εισόδου. Έτσι, αν είχαμε στατιστικά για τη συχνότητα εμφάνισης ζεύγων γραμμάτων θα μπορούσαμε πάλι να εφαρμόσουμε κανονικοποιημένα βάρη αντιστρόφως ανάλογα στην συχνότητα με την οποία προκύπτουν τα ζεύγη. \n",
    "\n",
    "### Βήμα 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('acceptor.fst', mode = 'w')\n",
    "count=1\n",
    "for word in tokens:\n",
    "    prev = \"<epsilon>\"\n",
    "    for i in range(len(word)):\n",
    "        l = word[i]\n",
    "        if i==len(word)-1: s=word\n",
    "        else: s=\"<epsilon>\"\n",
    "        if prev!=\"<epsilon>\":\n",
    "            g.write(str(count-1)+\" \"+str(count)+\" \"+l+\" \"+l +\" \"+'0''\\n')\n",
    "            count+=1\n",
    "        else:\n",
    "            g.write('0'+\" \"+str(count)+\" \"+l+\" \"+l+\" \"+'0'+'\\n')\n",
    "            count+=1\n",
    "        prev=l\n",
    "    g.write(str(count-1)+'\\n')\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α) Με τον ίδιο τρόπο δημιουργούμε με την εντολή\n",
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols acceptor.fst accept.fst\n",
    "\n",
    "τον εκτελέσιμο αποδοχέα.\n",
    "\n",
    "Ουσιαστικά για κάθε λέξη του λεξικού tokens εισάγουμε ένα μονοπάτι που αποδέχεται αυτή τη λέξη.\n",
    "\n",
    "β) Εφαρμόζουμε στη συνέχεια κατά σειρά τις ζητούμενες πράξεις πάνω στο accept.fst\n",
    "\n",
    "fstrmepsilon accept.fst acceptA.fst\n",
    "\n",
    "fstdeterminize acceptA.fst acceptB.fst\n",
    "\n",
    "fstminimize acceptB.fst acceptC.fst\n",
    "\n",
    "Η πρώτη από αυτές μετατρέπει το αυτόματο σε transducer χωρίς ε-καταστάσεις. Ο fstdeterminize μετατρέπει την είσοδο σε ντετερμινιστικό αυτόματο, ενώ η τελευταία εντολή ελαχιστοποιεί το αυτόματο μικραίνοντας κατά το δυνατό τον αριθμό των καταστάσεων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 7\n",
    "α) Δημιουργούμε τον ορθογράφο με την fstcompose\n",
    "\n",
    "fstcompose transducer.fst acceptC.fst min_dist_check.fst\n",
    "\n",
    "Δοκιμάζουμε επίσης να βάλουμε διαφορετικά βάρη για το αν τα γράμματα είναι σπάνια. Έτσι ώστε αν διαγράψουμε ή εισάγουμε ένα γράμμα που αλλάζει πολύ την λέξη να μην προτιμηθεί από το αυτόματο. Το ίδιο και αν αντικαταστήσουμε ένα σπάνιο γράμμα ή βάλουμε ένα σπάνιο γράμμα στη θέση ενός φωνήεντος που μπορεί να αλλάζει πολύ τη λέξη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('weights.fst', mode = 'w')\n",
    "weights = [30, 100, 100, 100, 40, 20, 35, 10, 15, 19, 32, 5, 25, 100, 60, 21, 23, 10, 7, 8, 14, 100, 19, 15, 13, 8,\n",
    "           24, 26, 9, 10, 100, 4, 100]\n",
    "\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[i], w=0)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=\"<epsilon>\", w=weights[i])\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=symbols[i], w=weights[i])\n",
    "    for j in range(len(symbols)):\n",
    "        if j!=i:\n",
    "            format_arc(\n",
    "                src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[j], w=weights[i]+weights[j])\n",
    "f.write('0')\n",
    "\n",
    "\n",
    "f.close()\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα σχολιάσουμε το ερώτημα για τη συμπεριφορά του weighted αυτόματου στο ερώτημα 8 που θα δούμε αναλυτικότερα τη συμποεριφορά του non-weighted πάνω σε δείγμα λέξεων. Φτιάχνουμε ωστόσο το fst με τις παρακάτω εντολές:\n",
    "\n",
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols weigts.fst third.fst\n",
    "\n",
    "fstcompose third.fst acceptC.fst weighted_edits.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Φτιάχνουμε το αρχείο εισόδου που είναι απλά η λέξη που θέλουμε να δώσουμε ως είσοδο σε μορφή fst.\n",
    "\n",
    "fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait input1.fst | dot -Tjpg > cit.jpg\n",
    "\n",
    "![title](cit.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = list('cit')\n",
    "def inp(letters,file):\n",
    "    s = 0\n",
    "\n",
    "    a = open(file, 'w')\n",
    "\n",
    "    for i in range(0, len(letters)):\n",
    "        a.write(str(s)+\" \"+str(s+1)+\" \"+letters[i]+\" \"+letters[i]+'\\n')\n",
    "        s += 1\n",
    "    a.write(str(s))\n",
    "    a.close()\n",
    "    \n",
    "inp(letters, 'input.fst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια κάνουμε compile το αρχείο εξόδου fst, κάνουμε fstcompose με τον ορθογράφο και αφού καλέσουμε την shortestPath για ν=5 βγάζουμε το αποτέλεσμα σε εικόνα για να είναι φανερό ποιες λέξεις είναι κοντινότερες και ποιές πράξεις εφαρμόστηκαν.\n",
    "\n",
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols input.fst input1.fst\n",
    "\n",
    "fstcompose input1.fst min_dist_check.fst fourth.fst\n",
    "\n",
    "fstshortestpath --nshortest=5 fourth.fst out.fst\n",
    "\n",
    "fstrmepsilon out.fst out.fst\n",
    "\n",
    "fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out.fst | dot -Tjpg > closest.jpg\n",
    "\n",
    "![title](closest.jpg)\n",
    "\n",
    "Από την εικόνα βλέπουμε ότι η λέξη 'cit' σύμφωνα με τον ορθογράφο είναι πιο κοντά και συγκεκριμένα απέχει από τις 5 λέξεις απόσταση 1 σύμφωνα με την απόσταση Levenshtein. Οι λέξεις είναι οι παρακάτω:\n",
    "\n",
    "hit\n",
    "\n",
    "sit \n",
    "\n",
    "city\n",
    "\n",
    "cut\n",
    "\n",
    "bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 8\n",
    "\n",
    "Αφού κατεβάσουμε το αρχείο επιλέγουμε τυχαία 20 από τις λανθασμένες λέξεις και τις βάζουμε σε μία λίστα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "words = ['vistors', 'magnifiscant', 'oppisite', 'cirtain', 'neccasary', 'acommodation', 'chalenges', 'lagh', 'perple', 'offten',\n",
    "        'somone', 'possition', 'sissors', 'speaical', 'experances', 'receit', 'compair', 'arrainged', 'poartry', 'arnt']\n",
    "\n",
    "correct = ['visitors', 'magnificent', 'opposite', 'certain', 'necessary', 'accommodation', 'challenges', 'laugh', 'purple',\n",
    "          'often', 'someone', 'position', 'scissors', 'special', 'experiences', 'receipt', 'compare', 'arranged', 'poetry', 'aunt']\n",
    "\n",
    "print(len(words))\n",
    "print(len(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για κάθε μία από αυτές δημιουργούμε το αντίστοιχο αρχείο εισόδου όπως στην περίπτωση της λέξης 'cit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "s = 'input'\n",
    "for i in range(len(words)):\n",
    "    files.append(s+str(i)+'.fst')\n",
    "    inp(words[i], files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για κάθε ένα από τα παραπάνω αρχεία εκτελώ το παρακάτω script το οποίο βάζει στο αρχείο a.txt την κοντινότερη προς την είσοδο λέξη που αποδέχεται ο ορθογράφος.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-b04c05bfdbd1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-b04c05bfdbd1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols $1 inputA.fst\u001b[0m\n\u001b[0m                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols $1 inputA.fst\n",
    "fstcompose inputA.fst min_dist_check.fst fourth.fst\n",
    "fstshortestpath --nshortest=1 fourth.fst out.fst\n",
    "fstrmepsilon out.fst out.fst\n",
    "fsttopsort out.fst out.fst\n",
    "fstprint out.fst word.txt\n",
    "cut -f4 word.txt > res.txt\n",
    "head -n -1 res.txt >> a.txt\n",
    "echo 'new_word' >> a.txt\n",
    "sed -i -e 's/<epsilon>//g' a.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επεξεργάζομαι στη συνέχεια το a.txt και βγάζω τις προβλέψεις"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word   correct word   prediction\n",
      "0        vistors       visitors     visitors\n",
      "1   magnifiscant    magnificent  magnificent\n",
      "2       oppisite       opposite     opposite\n",
      "3        cirtain        certain      certain\n",
      "4      neccasary      necessary    necessary\n",
      "5   acommodation  accommodation  combination\n",
      "6      chalenges     challenges      changes\n",
      "7           lagh          laugh        laugh\n",
      "8         perple         purple       purple\n",
      "9         offten          often        often\n",
      "10        somone        someone      someone\n",
      "11     possition       position     position\n",
      "12       sissors       scissors     scissors\n",
      "13      speaical        special      special\n",
      "14    experances    experiences  experiences\n",
      "15        receit        receipt      receipt\n",
      "16       compair        compare     complain\n",
      "17     arrainged       arranged     arranged\n",
      "18       poartry         poetry       portly\n",
      "19          arnt           aunt          art\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "f = open(\"a.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "word = \"\"\n",
    "for l in lines:\n",
    "    if l!='new_word\\n':\n",
    "        word+=l.rstrip()\n",
    "    else:\n",
    "        pred.append(word)\n",
    "        word=\"\"\n",
    "\n",
    "import pandas as pd\n",
    "d = pd.DataFrame({'word':words, 'correct word':correct, 'prediction':pred})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βλέπουμε ότι υπάρχουν αρκετές λάθος προβλέψεις, γεγονός που οφείλεται στα ορθογραφικά λάθη των λέξεων.\n",
    "Συγκεκριμένα, ο ορθογράφος εκτός από κάποιες λέξεις που δεν υπάρχουν στο λεξικό, επιλέγει σίγουρα λέξη με την ίδια ελάχιστη απόσταση από τη σωστή λέξη. Οι προβλέψεις αυτές έχουν ελάχιστη απόσταση από τη λέξη αλλά είναι αρκετά πιο \"ξένες\" τις σωστές γιατί αλλάζουν κάποια χαρακτηριστικά σύμφωνα των λέξεων, όπως στην περίπτωση του complain.\n",
    "\n",
    "Ωστόσο έχουν γίνει αρκετές σωστές προβλέψεις. Τέλος το λάθος είναι δικαιολογημένο στη λέξη \"arnt\" που είναι και μικρή λέξη και περιέχει το πιο ασυσχέτιστο προς τη σωστή λέξη \"r\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ασχολόυμαστε στη συνέχεια με τον weigthed ορθογράφο τον οποίο υλοποιούμε ομοίως με τον απλό:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-688f018f71f8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-688f018f71f8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols $1 inputA.fst\u001b[0m\n\u001b[0m                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols $1 inputA.fst\n",
    "fstcompose inputA.fst weighted_edits.fst fourth.fst\n",
    "fstshortestpath --nshortest=1 fourth.fst out.fst\n",
    "fstrmepsilon out.fst out.fst\n",
    "fsttopsort out.fst out.fst\n",
    "fstprint out.fst word.txt\n",
    "cut -f4 word.txt > res.txt\n",
    "head -n -1 res.txt >> w.txt\n",
    "echo 'new_word' >> w.txt\n",
    "sed -i -e 's/<epsilon>//g' w.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word   correct word   prediction\n",
      "0        vistors       visitors     visitors\n",
      "1   magnifiscant    magnificent  magnificent\n",
      "2       oppisite       opposite     opposite\n",
      "3        cirtain        certain      certain\n",
      "4      neccasary      necessary    necessary\n",
      "5   acommodation  accommodation       common\n",
      "6      chalenges     challenges      changes\n",
      "7           lagh          laugh        laugh\n",
      "8         perple         purple       purple\n",
      "9         offten          often        often\n",
      "10        somone        someone      someone\n",
      "11     possition       position     position\n",
      "12       sissors       scissors     scissors\n",
      "13      speaical        special      special\n",
      "14    experances    experiences  experiences\n",
      "15        receit        receipt      receipt\n",
      "16       compair        compare      compare\n",
      "17     arrainged       arranged     arranged\n",
      "18       poartry         poetry       poetry\n",
      "19          arnt           aunt        arent\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "f = open(\"w.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "word = \"\"\n",
    "for l in lines:\n",
    "    if l!='new_word\\n':\n",
    "        word+=l.rstrip()\n",
    "    else:\n",
    "        pred.append(word)\n",
    "        word=\"\"\n",
    "\n",
    "import pandas as pd\n",
    "d = pd.DataFrame({'word':words, 'correct word':correct, 'prediction':pred})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι συμπεριφέρεται παρόμοια με τον απλό ορθογράφο και ότι μάλλον δεν υπάρχει η αναγκαιότητα μεγάλων βαρών στον transducer. Υπενθυμίζουμε ότι κάποιες λέξεις είναι αρκετά ξένες ως προς τη σωστή γιατί δεν υπάρχουν στο αρχικό λεξικό. Ο σκοπός των διαφορετιών βαρών επιτυγχάνεται στο τελευταίο παράδειγμα όπου το arnt φέρνει πιο πολύ στη λέξη arent μια και απλά λείπει ένα \"e\". Βέβαια σύμφωνα με τα δεδομένα εισόδου η σωστή λέξη είναι η aunt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 9\n",
    "α) Ξαναγράφουμε την tokenize ώστε να επιστρέφει πρόταση. H readFile επιστρέφει κάθε γραμμή ως μία πρόταση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'bond', 'james', 'bond']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    news=\"\"\n",
    "    for a in s:\n",
    "        if a==' ' or a=='\\n': news+=' '\n",
    "        elif a.isalpha(): news+=a\n",
    "    return news.split()\n",
    "\n",
    "a=tokenize(\"My -name 9is Bo23nd, .././ JAMEs... @#Bond\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['one', 'nightit', 'was', 'on', 'the', 'twentieth', 'of', 'march', 'i', 'was', 'returning', 'from', 'a'], ['journey', 'to', 'a', 'patient', 'for', 'i', 'had', 'now', 'returned', 'to', 'civil', 'practice', 'when'], ['my', 'way', 'led', 'me', 'through', 'baker', 'street', 'as', 'i', 'passed', 'the', 'wellremembered'], ['door', 'which', 'must', 'always', 'be', 'associated', 'in', 'my', 'mind', 'with', 'my', 'wooing', 'and'], ['with', 'the', 'dark', 'incidents', 'of', 'the', 'study', 'in', 'scarlet', 'i', 'was', 'seized', 'with', 'a'], ['keen', 'desire', 'to', 'see', 'holmes', 'again', 'and', 'to', 'know', 'how', 'he', 'was', 'employing', 'his'], ['extraordinary', 'powers', 'his', 'rooms', 'were', 'brilliantly', 'lit', 'and', 'even', 'as', 'i'], ['looked', 'up', 'i', 'saw', 'his', 'tall', 'spare', 'figure', 'pass', 'twice', 'in', 'a', 'dark', 'silhouette'], ['against', 'the', 'blind', 'he', 'was', 'pacing', 'the', 'room', 'swiftly', 'eagerly', 'with', 'his']]\n"
     ]
    }
   ],
   "source": [
    "def readFile(pathname, preprocess=identity_process):\n",
    "    f = open(pathname, \"r\",encoding=\"utf8\")\n",
    "    mylist = []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        mylist.append(preprocess(line))\n",
    "    return mylist\n",
    "\n",
    "senten = readFile(p, tokenize)\n",
    "print(senten[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(senten, size=100, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106777342, 170524000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(senten, total_examples=len(senten), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = model.vector_size\n",
    "\n",
    "# get most similar words\n",
    "sim = model.wv.most_similar('holmes')\n",
    "\n",
    "# Convert to numpy 2d array (n_vocab x vector_size)\n",
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n",
    "    word2idx = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_matrix[i] = model.wv[model.wv.index2word[i]]\n",
    "        word2idx[model.wv.index2word[i]] = i\n",
    "    return embedding_matrix, model.wv.index2word, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clue\n",
      "[('clues', 0.6851588487625122), ('inkling', 0.6116672158241272), ('slightest_clue', 0.5809398889541626), ('know', 0.5430039763450623), ('foggiest_idea', 0.5362423658370972), ('hint', 0.5173760652542114), ('indication', 0.5164186954498291), ('faintest_idea', 0.511157751083374), ('wonder', 0.5002449154853821), ('hints', 0.492409884929657)]\n",
      "\n",
      "abandoned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abandoning', 0.6218770146369934), ('Abandoned', 0.580659031867981), ('abandonded', 0.5756361484527588), ('abandon', 0.552590548992157), ('derelict', 0.5503709316253662), ('abandonned', 0.5397235155105591), ('deserted', 0.5132448673248291), ('neglected', 0.5129300355911255), ('scrapped', 0.49730584025382996), ('discarded', 0.4953513443470001)]\n",
      "\n",
      "beside\n",
      "[('alongside', 0.5863358378410339), ('beneath', 0.5278612375259399), ('front', 0.5269509553909302), ('Beside', 0.5262925624847412), ('crouched_beside', 0.5186084508895874), ('astride', 0.5054730176925659), ('atop', 0.5051977038383484), ('underneath', 0.49534720182418823), ('overlooking', 0.49429577589035034), ('perched', 0.49401241540908813)]\n",
      "\n",
      "grasped\n",
      "[('grasp', 0.7057170867919922), ('grasping', 0.657967746257782), ('grasps', 0.6562371850013733), ('comprehended', 0.5534980297088623), ('embraced', 0.5434234142303467), ('understood', 0.5398856401443481), ('comprehend', 0.5192852020263672), ('realized', 0.4915735423564911), ('cottoned', 0.48466041684150696), ('sensed', 0.47712624073028564)]\n",
      "\n",
      "onto\n",
      "[('into', 0.5822286009788513), ('off', 0.5511479377746582), ('on', 0.4836716651916504), ('out', 0.4805983006954193), ('underneath', 0.4678601920604706), ('back', 0.46572452783584595), ('down', 0.44676482677459717), ('beneath', 0.4432578384876251), ('beside', 0.43407493829727173), ('along', 0.4246828854084015)]\n",
      "\n",
      "flying\n",
      "[('fly', 0.6795353889465332), ('flew', 0.6717677116394043), ('Flying', 0.6219244599342346), ('flies', 0.606692910194397), ('flight', 0.5564583539962769), ('flown', 0.5529292821884155), ('winging', 0.5423728227615356), ('flys', 0.5354082584381104), ('traveling', 0.5121676325798035), ('flyin', 0.5086062550544739)]\n",
      "\n",
      "including\n",
      "[('ranging', 0.616004467010498), ('included', 0.5976778864860535), ('inlcuding', 0.5968611240386963), ('include', 0.5880881547927856), ('incuding', 0.5638868808746338), ('assorted', 0.5455799698829651), ('Including', 0.5414242744445801), ('other', 0.5324223637580872), ('Among', 0.5304285287857056), ('includes', 0.5206085443496704)]\n",
      "\n",
      "smell\n",
      "[('smells', 0.827544093132019), ('smelling', 0.7983022928237915), ('odor', 0.7634782791137695), ('stench', 0.7501038908958435), ('aroma', 0.746222734451294), ('smelled', 0.7147930860519409), ('pungent_smell', 0.709552526473999), ('odors', 0.6990737915039062), ('scent', 0.6818752288818359), ('pungent_odor', 0.6662805080413818)]\n",
      "\n",
      "deeper\n",
      "[('deep', 0.6928302645683289), ('Deeper', 0.644614577293396), ('shallower', 0.5875859260559082), ('dig_deeper', 0.5739648938179016), ('deepest', 0.5636573433876038), ('deepening', 0.5306866765022278), ('farther', 0.5303624868392944), ('sharper', 0.5193043947219849), ('deepen', 0.5147820711135864), ('wider', 0.5065904855728149)]\n",
      "\n",
      "matter\n",
      "[('mater', 0.6651256084442139), ('matters', 0.610842227935791), ('issue', 0.5112205147743225), ('creatively_spelled', 0.4723905324935913), ('question', 0.4563380777835846), ('regardless', 0.45100659132003784), ('depends', 0.4346020221710205), ('boils_down', 0.41920751333236694), ('irrelevant', 0.41481494903564453), ('know', 0.4131358563899994)]\n",
      "\n",
      "owe\n",
      "[('owes', 0.768801212310791), ('owed', 0.7444417476654053), ('deserve', 0.48972436785697937), ('indebted', 0.48554694652557373), ('pay', 0.4832732081413269), ('paying', 0.4779888987541199), ('repay', 0.47607529163360596), ('paid', 0.45637720823287964), ('forgive', 0.45408105850219727), ('dunned', 0.4496442675590515)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test = ['clue']\n",
    "for i in range(10):\n",
    "    test.append(voc[random.choice(range(len(voc)))])\n",
    "\n",
    "for i in test:\n",
    "    print(i)\n",
    "    print(model.wv.most_similar(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχοντας φτιάξει το vocabulary του corpus επιλέγουμε εντελώς τυχαία 10 λέξεις από αυτό και τυπώνουμε τις πιο συσχετισμένες προς αυτές λέξεις.\n",
    "\n",
    "Παρατηρούμε ότι δεν είναι αρκετά επιτυχημένη η συσχέτιση των παρόμοιων λέξεων, εκτός ίσως αυτών που έχουν μεγαλύτερα ποσοστά συσχέτισης.\n",
    "Για παράδειγμα η λέξη clue (στοιχείο) είναι σημασιολογικά κοντά στη λέξη trace (ίχνος), αφού μιλάμε για υποθέσεις και σκηνές εγκλήματος στην περίπτωση του βιβλίου του Sherlock Holmes. Βέβαια οι υπόλοιπες λέξεις που φαίνονται ως αποτέλεσμα όπως το 'animal' δεν έχουν κάποια εμφανή σχέση με τη λέξη clue.\n",
    "\n",
    "Σε ό,τι αφορά τα υπόλοιπα αποτελέσματα δεν φαίνεται να έχει γίνει καλό match αν και οι λέξεις είναι πιο αφηρημένες (επίθετα, επιρρήματα, ...). Ασφαλώς βέβαια το μικρό window πιθανώς να μη βοηθάει στην καλύτερη εκπαίδευση του συστήματος. Ενδεχομένως βέβαια και ο όγκος των δεδομένων να μην είναι αρκετά μεγάλος ώστε να μας δώσει καλά αποτελέσματα.\n",
    "\n",
    "Αυξάνουμε αρχικά το window ώστε να συμπεριλάβουμε περισσότερες λέξεις στη σφαίρα συσχέτισης των λέξεων. Αυτό ενδεχομένως να βοηθήσει αν σκεφτούμε ότι μια λέξη στενά συνδεδεμένη με μια άλλη μπορεί να μην απέχει μόνο απόσταση 2 αλλά παραπάνω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(senten, size=100, window=10, workers=4)\n",
    "model.train(senten, total_examples=len(senten), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    print(i)\n",
    "    print(model.wv.most_similar(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε μια ελαφριά βελτίωση στις επιλεγμένες λέξεις. Για παράδειγμα στη λέξη \"εγκυκλοπέδια\" υπάρχει η λέξη \"παράγραφος\" που δεν υπήρχε πριν. Επίσης στη λέξη desk για παράδειγμα, υπάρχουν οι λέξεις table, opposite, triangular που ταιριάζουν σε ένα γραφείο και δεν υπήρχαν πριν. Συνοψίζοντας, παρατηρούμε ότι υπάρχει κάποια βελτίωση μετά την αύξηση του window_size. \n",
    "\n",
    "Αυξάνουμε τώρα τον αριθμό των epochs, ο οποίος ήταν ήδη 1000. Τον αυξάνουμε κατά μια τάξη μεγέθους και βλέπουμε αν αυτή η παράμετρος συμβάλλει σημαντικά στην εκπαίδευση του μοντέλου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(senten, size=100, window=10, workers=4)\n",
    "model.train(senten, total_examples=len(senten), epochs=5000)\n",
    "\n",
    "for i in test:\n",
    "    print(i)\n",
    "    print(model.wv.most_similar(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως είχαμε προβλέψει η αύξηση στον αριθμό των εποχών δε βοήθησε πολύ στη εύρεση πιο συσχετισμένων αποτελεσμάτων. Συγκεγκριμένα, έχουμε πάνω κάτω ίδια αποτελέσματα με πριν, ενώ έχουμε χάσει κάποια επιτυχημένα αποτελέσματα όπως για παράδειγμα το table του desk ή το \"powerful knock\" και το \"strength\" του convinced. \n",
    "\n",
    "Γενικά, συμπεραίνουμε ότι είναι καλή στρατηγική να χρησιμοποιούμε μεγαλύτερο window ώστε να αυξάνουμε την ακτίνα επιρροής και συμπεριλαμβάνουμε περισσότερα δεδομένα. Αντίθετα, ο μεγάλος αριθμός epochs δρα αρνητικά στην εκπαίδευση του μοντέλου έχοντας ως αποτέλεσμα ένα είδος overfitting που δίνει ασυσχέτιστα αποτελέσματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Μέρος 1ο : Ορθογράφος\n",
    "\n",
    "### Βήμα 10\n",
    "\n",
    "Με τη βοήθεια της βιβλιοθήκης collection βρίσκουμε τη συχνότητα των λέξεων στο αρχικό σύνολο λέξεων.\n",
    "Διαιρούμε ύστερα με το πλήθος των λέξεων του corpus για να βρούμε τις πιθανότητες εμφάνισης. Εφαρμόζουμε το ίδιο και για τα γράμματα τα οποία είναι όλα αποθκευμένα στη λίστα flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = dict(counter)\n",
    "\n",
    "letter_counter = dict(collections.Counter(flat_list))\n",
    "letter_counter\n",
    "for i in letter_counter:\n",
    "    letter_counter[i] /= len(flat_list)\n",
    "\n",
    "for i in word_counter:\n",
    "    word_counter[i] /= len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.07197618733799042,\n",
       " 'b': 0.017683757304160625,\n",
       " 'u': 0.03280831246430298,\n",
       " 'n': 0.0725253723474364,\n",
       " 'd': 0.04798778612538992,\n",
       " 'c': 0.039541320680110716,\n",
       " 'e': 0.12455516014234876,\n",
       " 'p': 0.02794253328061157,\n",
       " 'i': 0.07733623303018321,\n",
       " 'f': 0.015882430473177805,\n",
       " 'l': 0.05316110891437107,\n",
       " 'r': 0.07273406265102587,\n",
       " 'h': 0.026964983963797724,\n",
       " 's': 0.0764904881156364,\n",
       " 't': 0.06902157198717104,\n",
       " 'o': 0.05887263301260929,\n",
       " 'm': 0.025383331136593294,\n",
       " 'v': 0.011159439391942357,\n",
       " 'y': 0.017310311497737357,\n",
       " 'k': 0.009358112560959536,\n",
       " 'g': 0.030556653925574447,\n",
       " 'w': 0.01321339132727033,\n",
       " 'x': 0.002921664250252625,\n",
       " 'j': 0.0017793594306049821,\n",
       " 'z': 0.001109353719080884,\n",
       " 'q': 0.0015157506260709108,\n",
       " 'é': 0.00010983700188919643,\n",
       " 'æ': 3.295110056675893e-05,\n",
       " 'œ': 2.1967400377839286e-05,\n",
       " 'à': 1.0983700188919643e-05,\n",
       " 'ç': 1.0983700188919643e-05,\n",
       " 'è': 1.0983700188919643e-05,\n",
       " 'â': 1.0983700188919643e-05}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 11\n",
    "\n",
    "α) Υπολογίζουμε αρχικά τη ζητούμενη μέση τιμή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.086502198970369"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "word_mean = mean(word_counter[k] for k in word_counter)\n",
    "import math\n",
    "word_mean = -math.log(word_mean,10)\n",
    "word_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Φτιάχνουμε τον μετατροπέα όπως ακριβώς και στο βήμα 5 του προπαρασκευαστικού εργαστηρίου.\n",
    "\n",
    "Είναι πολύ σημαντικό να εφαρμόσουμε τον ορισμό του βάρους βάζοντας ως βάρος τον αντίθετο του log της τιμής που βρήκαμε παραπάνω. Έτσι το βάρος w θα είναι της ίδιας τάξης με τα βάρη του acceptor. Σε διαφορετική περίπτωση οι υψηλές τιμές βαρών του acceptor θα υπερίσχυαν στο συνολικό σύστημα επιλέγοντας έτσι απλά την πιο πολυσύχναστη λέξη (\"the\") και αγνοώντας τον transducer Levenshtein.\n",
    "\n",
    "Σημειώνουμε ότι τη μέση τιμή θα μπορούσαμε να την έχουμε βρει απλά ως 1/len(tokens) αφού οι πιθανότητες αθροίζουν στο 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word_unigram.fst', mode = 'w')\n",
    "\n",
    "def format_arc(src, dst, src_sym, dst_sym, w):\n",
    "    f.write(str(src)+\" \"+str(dst)+\" \"+src_sym+\" \"+dst_sym+\" \"+str(w)+'\\n')\n",
    "    return\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[i], w=0)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=\"<epsilon>\", w=word_mean)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=symbols[i], w=word_mean)\n",
    "    for j in range(len(symbols)):\n",
    "        if j!=i:\n",
    "            format_arc(\n",
    "                src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[j], w=word_mean)\n",
    "f.write('0')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "γ) Επαναλαμβάνουμε το ίδιο για το unigram μοντέλο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5185139398778873"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_mean = mean(letter_counter[k] for k in letter_counter)\n",
    "letter_mean = -math.log(letter_mean,10)\n",
    "f = open('letter_unigram.fst', mode = 'w')\n",
    "\n",
    "def format_arc(src, dst, src_sym, dst_sym, w):\n",
    "    f.write(str(src)+\" \"+str(dst)+\" \"+src_sym+\" \"+dst_sym+\" \"+str(w)+'\\n')\n",
    "    return\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[i], w=0)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=\"<epsilon>\", w=letter_mean)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=symbols[i], w=letter_mean)\n",
    "    for j in range(len(symbols)):\n",
    "        if j!=i:\n",
    "            format_arc(\n",
    "                src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[j], w=letter_mean)\n",
    "f.write('0')\n",
    "f.close()\n",
    "letter_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κάνουμε compile και τα 2 αρχεία με τις εντολές "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-528e4e0b320b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-528e4e0b320b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols word_unigram.fst word.fst\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols word_unigram.fst word.fst\n",
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols letter_unigram.fst letter.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η αφέλεια της παραπάνω μεθόδου οφείλεται στο γεγονός ότι έχουμε το ίδιο βάρος για κάθε πράξη που εφαρμόζουμε στη λέξη εισαγωγής. \n",
    "Όπως έχουμε αναφέρει και σε προηγούμενο ερώτημα, θα ήταν προτιμότερο να εξειδικεύσουμε τις τιμές για κάθε ζεύγος γραμμάτων ξεχωριστά ανάλογα με τη συχνότητα με την οποία συμβαίνουν αυτά τα λάθη. Έτσι θα μπορούσαμε να αντικαταστήσουμε την παραπάνω τιμή με την πιθανότητα να αντικατασταθεί ένα γράμμα με ένα άλλο κτλπ.\n",
    "\n",
    "Η απλότητα βέβαια της απόστασης Levenshtein, που μας δίνει τον ελάχιστο αριθμό πράξεων επί τον συντελεστή που δώσαμε είναι αρκετό για να κάνουμε string matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 12\n",
    "α) Ακολουθώντας τις οδηγίες βάζουμε το βάρος της λέξης στην πρώτη μετάβαση προς το μονοπάτι του fst που ορίζει την λέξη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('log_acceptor.fst', mode = 'w')\n",
    "count=1\n",
    "for word in tokens:\n",
    "    prev = \"<epsilon>\"\n",
    "    val = -math.log(word_counter[word],10)\n",
    "    for i in range(len(word)):\n",
    "        l = word[i]\n",
    "        if prev!=\"<epsilon>\":\n",
    "            g.write(str(count-1)+\" \"+str(count)+\" \"+l+\" \"+l +\" \"+'0'+'\\n')\n",
    "            count+=1\n",
    "        else:\n",
    "            g.write('0'+\" \"+str(count)+\" \"+l+\" \"+l+\" \"+str(val)+'\\n')\n",
    "            count+=1\n",
    "        prev=l\n",
    "    g.write(str(count-1)+'\\n')\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Ομοίως με το ερώτημα 6 έχουμε τα εξής."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-f6a7af0773d9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-f6a7af0773d9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols log_acceptor.fst word_acceptor.fst\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols log_acceptor.fst word_acceptor.fst\n",
    "fstrmepsilon word_acceptor.fst word_acceptor.fst\n",
    "fstdeterminize word_acceptor.fst word_acceptor.fst\n",
    "fstminimize word_acceptor.fst word_acceptor.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "γ) Κάνουμε το ίδιο και για τα γράμματα, μόνο που σε κάθε μετάβαση βάζουμε τον λογάριθμο της πιθανότητας του αντίστοιχου γράμματος με το οποίο γίνεται η μετάβαση στον acceptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('let_log_acceptor.fst', mode = 'w')\n",
    "count=1\n",
    "for word in tokens:\n",
    "    prev = \"<epsilon>\"\n",
    "    for i in range(len(word)):\n",
    "        l = word[i]\n",
    "        val = -math.log(letter_counter[l],10)\n",
    "        if prev!=\"<epsilon>\":\n",
    "            g.write(str(count-1)+\" \"+str(count)+\" \"+l+\" \"+l +\" \"+str(val)+'\\n')\n",
    "            count+=1\n",
    "        else:\n",
    "            g.write('0'+\" \"+str(count)+\" \"+l+\" \"+l+\" \"+str(val)+'\\n')\n",
    "            count+=1\n",
    "        prev=l\n",
    "    g.write(str(count-1)+'\\n')\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-9db8bd79d8b4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-9db8bd79d8b4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols let_log_acceptor.fst let_acceptor.fst\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols let_log_acceptor.fst let_acceptor.fst\n",
    "fstrmepsilon let_acceptor.fst let_acceptor.fst\n",
    "fstdeterminize let_acceptor.fst let_acceptor.fst\n",
    "fstminimize let_acceptor.fst let_acceptor.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 13\n",
    "α) Με την fstcompose δημιουργούμε τα αντίστοιχα binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-bb070daa894a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-bb070daa894a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstarcsort word.fst word.fst\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstarcsort word.fst word.fst\n",
    "fstarcsort word_acceptor.fst word_acceptor.fst\n",
    "fstcompose word.fst word_acceptor.fst word_checker.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Επαναλαμβάνουμε για το unigram μοντέλο:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-bcad5a021f1e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-bcad5a021f1e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstarcsort letter.fst letter.fst\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstarcsort let_acceptor.fst let_acceptor.fst\n",
    "fstcompose word.fst let_acceptor.fst let_checker.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "γ) Αναλύουμε στη συνέχεια τη συμπεριφορά των 2 αποτελεσμάτων με είσοδο τη λέξη \"cit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-f259e351defd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-f259e351defd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols input.fst word_input.fst\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols input.fst word_input.fst\n",
    "fstcompose word_input.fst word_checker.fst out.fst\n",
    "fstshortestpath --nshortest=3 out.fst out.fst\n",
    "fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out.fst | dot -Tjpg > cit_log.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-a384cd7f5404>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-a384cd7f5404>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompose word_input.fst letter_checker.fst out1.fst\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompose word_input.fst let_checker.fst out1.fst\n",
    "fstshortestpath --nshortest=3 out1.fst out1.fst\n",
    "fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out1.fst | dot -Tjpg > cit_unigram.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα 3 shortest paths για τον word model speller φαίνονται στο παρακάτω διάγραμμα.\n",
    "![title](cit_log.jpg)\n",
    "\n",
    "Αντίστοιχα για τον unigram spell checker το αποτέλεσμα είναι το εξής:\n",
    "\n",
    "![title](cit_unigram.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ουσιαστικά για τον 1ο speller οι κοντινότερες λέξεις είναι οι \"it\", \"pit\", \"cut\" ενώ για τον δεύτερο οι λέξεις \"it\", \"sit\", \"lit\".\n",
    "Αυτή η συμπεριφορά είναι και ενδεικτική του τρόπου λειτουργίας των 2 μηχανών, μια και η πρώτη καθορίζεται ως ένα βαθμό από τη συχνότητα των λέξεων που ενδεχομένως να αντικαταστήσουν την λέξη-είσοδο, ενώ η δεύτερη καθορίζεται περισσότερο από τα συχνά γράμματα που μπορεί να διορθώσουν ένα λάθος. \n",
    "\n",
    "Έτσι για παράδειγμα στην πρώτη περίπτωση οι λέξεις pit, cut είναι πιο συχνές από τις sit, lit, ενώ αντίθετα τα γράμματα \"l\", \"s\", είναι πιο πολυσύχναστα από τα \"p\", \"u\" που χρησιμοποιήθηκαν ως αντικατάσταση στην πρώτη περίπτωση. Αυτό αποτυπώνεται και στα βάρη των παραπάνω fst όπου οι αντικατστάσεις κοστίζουν."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 14\n",
    "\n",
    "α) Βάζω την είσοδο σε ένα test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test.txt\", \"r\")\n",
    "lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contenpted contented\n",
      "contende contented\n",
      "contended contented\n",
      "contentid contented\n",
      "begining beginning\n",
      "problam problem\n",
      "proble problem\n",
      "promblem problem\n",
      "proplen problem\n",
      "dirven driven\n",
      "exstacy ecstasy\n",
      "ecstacy ecstasy\n",
      "guic juice\n",
      "juce juice\n",
      "jucie juice\n",
      "juise juice\n",
      "juse juice\n",
      "localy locally\n",
      "compair compare\n",
      "pronounciation pronunciation\n",
      "transportibility transportability\n",
      "miniscule minuscule\n",
      "independant independent\n",
      "independant independent\n",
      "aranged arranged\n",
      "arrainged arranged\n",
      "poartry poetry\n",
      "poertry poetry\n",
      "poetre poetry\n",
      "poety poetry\n",
      "powetry poetry\n",
      "leval level\n",
      "basicaly basically\n",
      "triangulaur triangular\n",
      "unexpcted unexpected\n",
      "unexpeted unexpected\n",
      "unexspected unexpected\n",
      "stanerdizing standardizing\n",
      "varable variable\n",
      "futher further\n",
      "monitering monitoring\n",
      "biscits biscuits\n",
      "biscutes biscuits\n",
      "biscuts biscuits\n",
      "bisquits biscuits\n",
      "buiscits biscuits\n",
      "buiscuts biscuits\n",
      "avaible available\n",
      "seperate separate\n",
      "neccesary necessary\n",
      "necesary necessary\n",
      "neccesary necessary\n",
      "necassary necessary\n",
      "necassery necessary\n",
      "neccasary necessary\n",
      "defenition definition\n",
      "receit receipt\n",
      "receite receipt\n",
      "reciet receipt\n",
      "recipt receipt\n",
      "remine remind\n",
      "remined remind\n",
      "inetials initials\n",
      "inistals initials\n",
      "initails initials\n",
      "initals initials\n",
      "intials initials\n",
      "magnificnet magnificent\n",
      "magificent magnificent\n",
      "magnifcent magnificent\n",
      "magnifecent magnificent\n",
      "magnifiscant magnificent\n",
      "magnifisent magnificent\n",
      "magnificant magnificent\n",
      "annt aunt\n",
      "anut aunt\n",
      "arnt aunt\n",
      "intial initial\n",
      "ther there\n",
      "experances experiences\n",
      "biult built\n",
      "totaly totally\n",
      "undersand understand\n",
      "undistand understand\n",
      "southen southern\n",
      "definately definitely\n",
      "difinately definitely\n",
      "fisited visited\n",
      "viseted visited\n",
      "vistid visited\n",
      "vistied visited\n",
      "volantry voluntary\n",
      "ment meant\n",
      "recieve receive\n",
      "sorces sources\n",
      "wether whether\n",
      "usefull useful\n",
      "litriture literature\n",
      "valubale valuable\n",
      "valuble valuable\n",
      "desicate desiccate\n",
      "dessicate desiccate\n",
      "dessiccate desiccate\n",
      "clearical clerical\n",
      "spledid splendid\n",
      "splended splendid\n",
      "splened splendid\n",
      "splended splendid\n",
      "beetween between\n",
      "completly completely\n",
      "acount account\n",
      "cemetary cemetery\n",
      "semetary cemetery\n",
      "speaical special\n",
      "specail special\n",
      "specal special\n",
      "speical special\n",
      "lates latest\n",
      "latets latest\n",
      "latiest latest\n",
      "latist latest\n",
      "perhapse perhaps\n",
      "rember remember\n",
      "remeber remember\n",
      "rememmer remember\n",
      "rermember remember\n",
      "chaper chapter\n",
      "chaphter chapter\n",
      "chaptur chapter\n",
      "cak cake\n",
      "vairious various\n",
      "febuary february\n",
      "pertend pretend\n",
      "protend pretend\n",
      "prtend pretend\n",
      "pritend pretend\n",
      "chosing choosing\n",
      "rote wrote\n",
      "wote wrote\n",
      "particulaur particular\n",
      "awfall awful\n",
      "afful awful\n",
      "arragment arrangement\n",
      "chalenges challenges\n",
      "chalenges challenges\n",
      "lagh laugh\n",
      "lauf laugh\n",
      "laught laugh\n",
      "lugh laugh\n",
      "ofen often\n",
      "offen often\n",
      "offten often\n",
      "ofton often\n",
      "somone someone\n",
      "personnell personnel\n",
      "uneque unique\n",
      "diagrammaticaally diagrammatically\n",
      "discription description\n",
      "poims poems\n",
      "pomes poems\n",
      "perple purple\n",
      "perpul purple\n",
      "poarple purple\n",
      "descide decide\n",
      "articals articles\n",
      "possition position\n",
      "extented extended\n",
      "hierachial hierarchal\n",
      "realy really\n",
      "relley really\n",
      "relly really\n",
      "voteing voting\n",
      "comittee committee\n",
      "wantid wanted\n",
      "wonted wanted\n",
      "benifits benefits\n",
      "defenitions definitions\n",
      "scisors scissors\n",
      "sissors scissors\n",
      "levals levels\n",
      "paralel parallel\n",
      "paralell parallel\n",
      "parrallel parallel\n",
      "parralell parallel\n",
      "parrallell parallel\n",
      "accomodation accommodation\n",
      "acommodation accommodation\n",
      "acomodation accommodation\n",
      "planed planned\n",
      "hierchy hierarchy\n",
      "transfred transferred\n",
      "muinets minutes\n",
      "aranging arrangeing\n",
      "accesing accessing\n",
      "stomac stomach\n",
      "stomache stomach\n",
      "stomec stomach\n",
      "stumache stomach\n",
      "unfortunatly unfortunately\n",
      "conciderable considerable\n",
      "acess access\n",
      "singulaur singular\n",
      "scarcly scarcely\n",
      "scarecly scarcely\n",
      "scarely scarcely\n",
      "scarsely scarcely\n",
      "questionaire questionnaire\n",
      "experance experience\n",
      "experiance experience\n",
      "possable possible\n",
      "reafreshment refreshment\n",
      "refreshmant refreshment\n",
      "refresment refreshment\n",
      "refressmunt refreshment\n",
      "embaras embarrass\n",
      "embarass embarrass\n",
      "vistors visitors\n",
      "auxillary auxiliary\n",
      "descided decided\n",
      "benifit benefit\n",
      "concider consider\n",
      "failes fails\n",
      "carrer career\n",
      "occurence occurrence\n",
      "occurence occurrence\n",
      "cirtain certain\n",
      "poame poem\n",
      "liew lieu\n",
      "astablishing establishing\n",
      "establising establishing\n",
      "diffrent different\n",
      "lones loans\n",
      "extreamly extremely\n",
      "addresable addressable\n",
      "galery gallery\n",
      "gallary gallery\n",
      "gallerry gallery\n",
      "gallrey gallery\n",
      "centraly centrally\n",
      "familes families\n",
      "bicycal bicycle\n",
      "bycicle bicycle\n",
      "bycycle bicycle\n",
      "choise choice\n",
      "opisite opposite\n",
      "oppasite opposite\n",
      "oppesite opposite\n",
      "oppisit opposite\n",
      "oppisite opposite\n",
      "opposit opposite\n",
      "oppossite opposite\n",
      "oppossitte opposite\n",
      "cartains curtains\n",
      "certans curtains\n",
      "courtens curtains\n",
      "cuaritains curtains\n",
      "curtans curtains\n",
      "curtians curtains\n",
      "curtions curtains\n",
      "adress address\n",
      "adres address\n",
      "liaision liaison\n",
      "liason liaison\n",
      "managment management\n",
      "inconvienient inconvenient\n",
      "inconvient inconvenient\n",
      "inconvinient inconvenient\n",
      "vairiant variant\n",
      "supercede supersede\n",
      "superceed supersede\n"
     ]
    }
   ],
   "source": [
    "misspelled = []\n",
    "truth = []\n",
    "for l in lines:\n",
    "    s = l.split(\":\")\n",
    "    correct = s[0]\n",
    "    wr = s[1]\n",
    "    wrong = wr.split(\" \")\n",
    "    for i in range(1,len(wrong)):\n",
    "        misspelled.append(wrong[i].strip('\\n'))\n",
    "        truth.append(correct)\n",
    "for i in range(len(misspelled)):\n",
    "    print(misspelled[i]+\" \"+truth[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Τρέχουμε στη συνέχεια στο τερματικό το εξής script ώστε να μπορούμε ταυτόχρονα να τρέξουμε και bash εντολές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['visitors', 'magnificent', 'opposite', 'certain', 'necessary', 'combination', 'changes', 'laugh', 'purple', 'often', 'someone', 'position', 'scissors', 'special', 'experiences', 'receipt', 'complain', 'arranged', 'portly', 'art', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in misspelled:\n",
    "    inp(i, \"input.fst\")\n",
    "    os.system(\"fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols input.fst inputA.fst\")\n",
    "    os.system(\"fstcompose inputA.fst word_checker.fst fourth.fst\")\n",
    "    os.system(\"fstshortestpath --nshortest=1 fourth.fst out.fst\")\n",
    "    os.system(\"fstrmepsilon out.fst out.fst\")\n",
    "    os.system(\"fsttopsort out.fst out.fst\")\n",
    "    os.system(\"fstprint out.fst word.txt\")\n",
    "    os.system(\"cut -f4 word.txt > res.txt\")\n",
    "    os.system(\"head -n -1 res.txt >> a.txt\")\n",
    "    os.system(\"echo 'new_word' >> a.txt\")\n",
    "    os.system(\"sed -i -e 's/<epsilon>//g' a.txt\")\n",
    "\n",
    "\n",
    "pred = []\n",
    "f = open(\"a.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "word = \"\"\n",
    "for l in lines:\n",
    "    if l!='new_word\\n':\n",
    "        word+=l.rstrip()\n",
    "    else:\n",
    "        pred.append(word)\n",
    "        word=\"\"\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ως αποτέλεσμα έχουμε το παρακάτω pred με βάση το οποίο βρίσκουμε το ποσοστό επιτυχίας σε όλες τις λέξεις αλλά και αν κρατήσουμε μόνο τις λέξεις που υπάρχουν στο λεξικό μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "270\n",
      "prediction score on total test set is 56.666666666666664 %\n"
     ]
    }
   ],
   "source": [
    "pred = ['consented', 'continue', 'continued', 'contents', 'beginning', 'problem', 'problem', 'problem', 'people', 'given', 'eustace', 'eustace', 'guns', 'june', 'judge', 'just', 'just', 'local', 'company', 'provocation', 'responsibility', 'miniature', 'independent', 'independent', 'arranged', 'arranged', 'party', 'poetry', 'poetry', 'poetry', 'poetry', 'legal', 'easily', 'triangular', 'unexpected', 'unexpected', 'unexpected', 'standing', 'variable', 'father', 'loitering', 'biscuits', 'biscuits', 'biscuits', 'biscuits', 'biscuits', 'biscuits', 'available', 'separate', 'necessary', 'necessary', 'necessary', 'necessary', 'necessary', 'necessary', 'desertion', 'recent', 'receive', 'secret', 'receipt', 'remind', 'remained', 'initials', 'initials', 'initials', 'initials', 'initials', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'aint', 'ant', 'art', 'until', 'the', 'experiences', 'but', 'total', 'understand', 'understand', 'southern', 'definitely', 'definitely', 'visited', 'visited', 'visit', 'visited', 'country', 'went', 'relieve', 'forces', 'whether', 'useful', 'literature', 'valuable', 'valuable', 'delicate', 'delicate', 'delicate', 'clearing', 'splendid', 'splendid', 'opened', 'splendid', 'between', 'completely', 'account', 'century', 'secretary', 'special', 'special', 'special', 'special', 'late', 'late', 'latest', 'latest', 'perhaps', 'member', 'remember', 'remember', 'remember', 'chapter', 'chapter', 'chapter', 'can', 'various', 'february', 'parted', 'pretend', 'pretend', 'pretend', 'closing', 'rose', 'note', 'particular', 'wall', 'awful', 'fragment', 'changes', 'changes', 'laugh', 'last', 'caught', 'laugh', 'open', 'often', 'often', 'often', 'someone', 'personal', 'unique', 'practically', 'description', 'point', 'comes', 'people', 'peril', 'purple', 'decide', 'articles', 'position', 'extended', 'special', 'really', 'really', 'really', 'nothing', 'committee', 'wanted', 'wanted', 'benefit', 'deletions', 'scissors', 'scissors', 'legal', 'parallel', 'parallel', 'parallel', 'parallel', 'parallel', 'accumulation', 'composition', 'abomination', 'planet', 'fierce', 'transferred', 'mines', 'hanging', 'accepting', 'stomachs', 'stomachs', 'some', 'stomachs', 'unfortunately', 'considerable', 'access', 'singular', 'scarcely', 'scarcely', 'scarcely', 'scarcely', 'questionable', 'experience', 'experience', 'possible', 'refreshed', 'refreshed', 'represent', 'represent', 'mars', 'brass', 'visitors', 'artillery', 'decided', 'benefit', 'consider', 'failed', 'career', 'occurrence', 'occurrence', 'certain', 'some', 'view', 'establishing', 'establishing', 'different', 'ones', 'extremely', 'advisable', 'salary', 'salary', 'valley', 'valley', 'central', 'families', 'bicycle', 'bicycle', 'bicycle', 'chaise', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'curtains', 'certain', 'countess', 'curtains', 'curtains', 'martians', 'curious', 'address', 'acres', 'division', 'reason', 'management', 'inconvenience', 'incoherent', 'inconvenience', 'vacant', 'supersede', 'superseded']\n",
    "print(len(pred))\n",
    "print(len(truth))\n",
    "\n",
    "a = sum(pred[i]==truth[i] for i in range(len(pred)))\n",
    "print(\"prediction score on total test set is\", a/len(pred)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction score counting only words inside our dictionary is 77.66497461928934 %\n"
     ]
    }
   ],
   "source": [
    "score=0\n",
    "total=0\n",
    "for i in range(len(truth)):\n",
    "    if truth[i] in tokens:\n",
    "        if pred[i]==truth[i]: \n",
    "            score+=1\n",
    "        total+=1\n",
    "print(\"prediction score counting only words inside our dictionary is\", score/total*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτή είναι και η τιμή που λαμβάνουμε υπόψη στο score. Εφαρμόζουμε τώρα ακριβώς τα ίδια βήματα για τον unigram speller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in misspelled:\n",
    "    inp(i, \"input.fst\")\n",
    "    os.system(\"fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols input.fst inputA.fst\")\n",
    "    os.system(\"fstcompose inputA.fst let_checker.fst fourth.fst\")\n",
    "    os.system(\"fstshortestpath --nshortest=1 fourth.fst out.fst\")\n",
    "    os.system(\"fstrmepsilon out.fst out.fst\")\n",
    "    os.system(\"fsttopsort out.fst out.fst\")\n",
    "    os.system(\"fstprint out.fst word.txt\")\n",
    "    os.system(\"cut -f4 word.txt > res.txt\")\n",
    "    os.system(\"head -n -1 res.txt >> a.txt\")\n",
    "    os.system(\"echo 'new_word' >> a.txt\")\n",
    "    os.system(\"sed -i -e 's/<epsilon>//g' a.txt\")\n",
    "\n",
    "\n",
    "pred = []\n",
    "f = open(\"a.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "word = \"\"\n",
    "for l in lines:\n",
    "    if l!='new_word\\n':\n",
    "        word+=l.rstrip()\n",
    "    else:\n",
    "        pred.append(word)\n",
    "        word=\"\"\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction score on total test set is 50.74074074074074 %\n",
      "prediction score counting only words inside our dictionary is 69.54314720812182 %\n"
     ]
    }
   ],
   "source": [
    "pred=['consented', 'contents', 'intended', 'contents', 'beginning', 'problem', 'role', 'problem', 'people', 'siren', 'eustace', 'eustace', 'gun', 'june', 'june', 'use', 'use', 'local', 'compare', 'provocation', 'responsibility', 'miniature', 'independent', 'independent', 'arranged', 'arranged', 'party', 'poetry', 'poetry', 'piety', 'poetry', 'legal', 'scaly', 'triangular', 'unexpected', 'unexpected', 'unexpected', 'standing', 'variable', 'father', 'loitering', 'biscuits', 'disputes', 'biscuits', 'biscuits', 'biscuits', 'biscuits', 'avail', 'separate', 'necessary', 'necessary', 'necessary', 'necessary', 'necessary', 'necessary', 'desertion', 'recent', 'receive', 'resist', 'receipt', 'remind', 'remind', 'initials', 'initials', 'initials', 'initials', 'initials', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'ant', 'ant', 'art', 'india', 'her', 'experiences', 'bit', 'total', 'understand', 'understand', 'southern', 'definitely', 'definitely', 'visited', 'visited', 'visit', 'tied', 'pantry', 'men', 'relieve', 'forces', 'weather', 'useful', 'literature', 'value', 'value', 'delicate', 'delicate', 'delicate', 'clearing', 'splendid', 'splendid', 'speed', 'splendid', 'between', 'completely', 'count', 'century', 'smear', 'special', 'special', 'special', 'special', 'late', 'lets', 'latest', 'latest', 'perhaps', 'member', 'remember', 'remember', 'remember', 'chapter', 'chapter', 'chapter', 'ak', 'various', 'february', 'parted', 'pretend', 'pretend', 'pretend', 'closing', 'rate', 'woe', 'particular', 'fall', 'awful', 'armament', 'changes', 'changes', 'lash', 'la', 'laugh', 'ugh', 'open', 'often', 'often', 'often', 'someone', 'personal', 'unique', 'dramatic', 'description', 'pins', 'poses', 'people', 'peril', 'purple', 'decide', 'article', 'position', 'extended', 'special', 'real', 'rely', 'rely', 'noting', 'committee', 'wanted', 'wanted', 'benefit', 'deletions', 'scissors', 'scissors', 'leads', 'parallel', 'parallel', 'parallel', 'parallel', 'parallel', 'accumulation', 'consolation', 'abomination', 'plane', 'perch', 'transpired', 'mines', 'ringing', 'access', 'star', 'stomachs', 'some', 'stomachs', 'unfortunately', 'considerable', 'access', 'singular', 'scarcely', 'scarcely', 'scarcely', 'scarcely', 'questionable', 'experience', 'experience', 'passable', 'represent', 'refresh', 'represent', 'represent', 'ears', 'brass', 'visitors', 'allay', 'decided', 'benefit', 'consider', 'files', 'career', 'occurrence', 'occurrence', 'certain', 'ome', 'lie', 'establishing', 'establishing', 'different', 'ones', 'extremely', 'advisable', 'gale', 'allay', 'alley', 'alley', 'central', 'families', 'bicycle', 'bicycle', 'bicycle', 'chose', 'site', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'curtains', 'certain', 'countess', 'curtains', 'curtains', 'martians', 'curious', 'dress', 'acres', 'division', 'bison', 'management', 'convenient', 'invent', 'convenient', 'vacant', 'supersede', 'superseded']\n",
    "\n",
    "a = sum(pred[i]==truth[i] for i in range(len(pred)))\n",
    "print(\"prediction score on total test set is\", a/len(pred)*100, '%')\n",
    "score=0\n",
    "total=0\n",
    "for i in range(len(truth)):\n",
    "    if truth[i] in tokens:\n",
    "        if pred[i]==truth[i]: \n",
    "            score+=1\n",
    "        total+=1\n",
    "print(\"prediction score counting only words inside our dictionary is\", score/total*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "γ) Παρατηρούμε καταρχάς ότι τα ποσοστά δεν είναι εξαιρετικά ψηλά εξαιτίας είτε της απουσίας λέξεων στο corpus στην πρώτη περίπτωση είτε λόγω της πολύ κοντινής απόστασης κάποιων λέξεων. Συγκριτικά όμως ο word level speller έχει καλύτερο performance μια και λόγω των λίγων λέξεων του corpus τείνει προς τις πιο χρησιμοποιημένες λέξεις, γεγονός που αποδίδει φυσικά καλύτερα από το να προσανατολίζομαι προς τα πιο συχνά γράμματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 15\n",
    "\n",
    "Αυτό που διαφοροποιείται με τις προηγούμενες περιπτώσεις είναι οι πιθανότητες μετάβασης του acceptor. Αρχικά τις υπολογίζω:\n",
    "\n",
    "Συμπεριλαμβάνω και τις πιθανότητες μια λέξη να ξεκινάει από ένα συγκεκριμένο γράμμα:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prob = np.zeros((len(symbols)+1,len(symbols)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000e+00 5.800e+01 8.400e+01 1.000e+00 0.000e+00 2.100e+01 4.200e+02\n",
      " 2.000e+01 3.000e+00 3.000e+01 3.590e+02 1.760e+02 1.370e+02 1.050e+02\n",
      " 7.440e+02 0.000e+00 1.000e+00 6.900e+01 0.000e+00 8.100e+01 0.000e+00\n",
      " 6.690e+02 3.000e+00 1.820e+02 5.970e+02 0.000e+00 1.560e+02 1.120e+02\n",
      " 2.050e+02 4.800e+01 1.900e+02 0.000e+00 1.469e+03 0.000e+00]\n",
      "91044\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for l in tokens:\n",
    "    prev = \"\"\n",
    "    for c in l:\n",
    "        if prev!=\"\":\n",
    "            prob[symbols.index(prev)+1][symbols.index(c)+1]+=1\n",
    "        else:\n",
    "            prob[0][symbols.index(c)+1]+=1\n",
    "        prev = c\n",
    "        total+=1\n",
    "print(prob[23,:])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 9.45696586e-03 8.45744915e-04 5.38201309e-04\n",
      " 0.00000000e+00 5.54676860e-03 1.66403058e-02 5.54676860e-03\n",
      " 1.02148412e-03 6.89776372e-03 7.06251922e-03 2.69100655e-03\n",
      " 2.40543034e-03 4.23970827e-03 5.37102939e-03 0.00000000e+00\n",
      " 1.09837002e-04 1.99903343e-03 0.00000000e+00 4.89873028e-03\n",
      " 0.00000000e+00 7.74350863e-03 0.00000000e+00 7.18333992e-03\n",
      " 3.20724046e-03 0.00000000e+00 5.83234480e-03 1.25763367e-02\n",
      " 8.43548175e-03 7.47989983e-03 4.28364307e-04 0.00000000e+00\n",
      " 5.79939370e-03 8.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(symbols)):\n",
    "    for j in range(len(symbols)):\n",
    "        prob[i][j]=prob[i][j]/total\n",
    "print(prob[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φτιάχνουμε στη συνέχεια τον acceptor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "g = open('bigram_acceptor.fst', mode = 'w')\n",
    "count=1\n",
    "for word in tokens:\n",
    "    prev = \"<epsilon>\"\n",
    "    for i in range(len(word)):\n",
    "        l = word[i]\n",
    "        if prev!=\"<epsilon>\":\n",
    "            val = -math.log(prob[symbols.index(prev)+1][symbols.index(l)+1],10)\n",
    "            g.write(str(count-1)+\" \"+str(count)+\" \"+l+\" \"+l +\" \"+str(val)+'\\n')\n",
    "            count+=1\n",
    "        else:\n",
    "            val = -math.log(prob[0][symbols.index(l)+1],10)\n",
    "            g.write('0'+\" \"+str(count)+\" \"+l+\" \"+l+\" \"+str(val)+'\\n')\n",
    "            count+=1\n",
    "        prev=l\n",
    "    g.write(str(count-1)+'\\n')\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "bi_mean = np.mean(prob)\n",
    "bi_mean = -math.log(bi_mean,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.039438719708233\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "for i in range(len(symbols)):\n",
    "    for j in range(len(symbols)):\n",
    "        tot+=prob[i][j]\n",
    "bi_mean=tot/len(symbols)/len(symbols)\n",
    "bi_mean = -math.log(bi_mean,10)\n",
    "print(bi_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τελικά, ούτως ώστε να λαμβάνεται υπόψη ο transducer βάζουμε σχετικά μεγάλο βάρος για το edit πχ 30. \n",
    "Έτσι κάνουμε compose κατά τα γνωστά έχοντας εφαρμόσει τα στατιστικά αποτελέσματα στον acceptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('bi_mean.fst', mode = 'w')\n",
    "\n",
    "def format_arc(src, dst, src_sym, dst_sym, w):\n",
    "    f.write(str(src)+\" \"+str(dst)+\" \"+src_sym+\" \"+dst_sym+\" \"+str(w)+'\\n')\n",
    "    return\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[i], w=0)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=symbols[i], dst_sym=\"<epsilon>\", w=30)\n",
    "    format_arc(\n",
    "        src=0, dst=0, src_sym=\"<epsilon>\", dst_sym=symbols[i], w=30)\n",
    "    for j in range(len(symbols)):\n",
    "        if j!=i:\n",
    "            format_arc(\n",
    "                src=0, dst=0, src_sym=symbols[i], dst_sym=symbols[j], w=30)\n",
    "f.write('0')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-100-2f8b243070e0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-100-2f8b243070e0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols bigram_acceptor.fst bigram.fst\u001b[0m\n\u001b[0m                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols bigram_acceptor.fst bigram.fst\n",
    "fstrmepsilon bigram.fst bigram.fst\n",
    "fstdeterminize bigram.fst bigram.fst\n",
    "fstminimize bigram.fst bigram.fst\n",
    "\n",
    "fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols bi_mean.fst bi_trans.fst\n",
    "fstarcsort bigram.fst bigram.fst\n",
    "fstcompose bi_trans.fst bigram.fst bigram_checker.fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στη συνέχεια τρέχουμε το script με το test set για να δούμε αν έχει βελτιωμένα ποσοστά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in misspelled:\n",
    "    inp(i, \"input.fst\")\n",
    "    os.system(\"fstcompile --isymbols=chars.syms --osymbols=chars.syms --keep_isymbols --keep_osymbols input.fst inputA.fst\")\n",
    "    os.system(\"fstcompose inputA.fst bigram_checker.fst fourth.fst\")\n",
    "    os.system(\"fstshortestpath --nshortest=1 fourth.fst out.fst\")\n",
    "    os.system(\"fstrmepsilon out.fst out.fst\")\n",
    "    os.system(\"fsttopsort out.fst out.fst\")\n",
    "    os.system(\"fstprint out.fst word.txt\")\n",
    "    os.system(\"cut -f4 word.txt > res.txt\")\n",
    "    os.system(\"head -n -1 res.txt >> a.txt\")\n",
    "    os.system(\"echo 'new_word' >> a.txt\")\n",
    "    os.system(\"sed -i -e 's/<epsilon>//g' a.txt\")\n",
    "\n",
    "\n",
    "pred = []\n",
    "f = open(\"a.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "word = \"\"\n",
    "for l in lines:\n",
    "    if l!='new_word\\n':\n",
    "        word+=l.rstrip()\n",
    "    else:\n",
    "        pred.append(word)\n",
    "        word=\"\"\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction score on total test set is 52.59259259259259 %\n",
      "prediction score counting only words inside our dictionary is 72.08121827411168 %\n"
     ]
    }
   ],
   "source": [
    "pred=[]\n",
    "pred=['consented', 'contents', 'intended', 'contents', 'beginning', 'problem', 'problem', 'problem', 'proper', 'siren', 'exact', 'eustace', 'gun', 'june', 'june', 'use', 'use', 'local', 'compare', 'provocation', 'responsibility', 'miniature', 'independent', 'independent', 'arranged', 'arranged', 'party', 'poetry', 'poetry', 'piety', 'poetry', 'level', 'scaly', 'triangular', 'unexpected', 'unexpected', 'unexpected', 'standing', 'variable', 'father', 'loitering', 'biscuits', 'disputes', 'biscuits', 'biscuits', 'biscuits', 'biscuits', 'avail', 'separate', 'necessary', 'necessary', 'necessary', 'necessary', 'necessary', 'necessary', 'desertion', 'recent', 'receive', 'exit', 'receipt', 'remind', 'remind', 'initials', 'initials', 'initials', 'initials', 'initials', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'magnificent', 'ant', 'ant', 'ant', 'india', 'her', 'experiences', 'bit', 'total', 'understand', 'understand', 'southend', 'definitely', 'definitely', 'visited', 'visited', 'visit', 'visited', 'pantry', 'men', 'relieve', 'forces', 'weather', 'useful', 'literature', 'valuable', 'valuable', 'delicate', 'delicate', 'delicate', 'clearing', 'splendid', 'splendid', 'spend', 'splendid', 'between', 'completely', 'count', 'century', 'sedentary', 'special', 'special', 'special', 'special', 'late', 'late', 'latest', 'latest', 'perhaps', 'member', 'remember', 'remember', 'remember', 'chapter', 'chapter', 'chapter', 'ak', 'various', 'february', 'extend', 'pretend', 'pretend', 'pretend', 'chasing', 'rate', 'woe', 'particular', 'wall', 'awful', 'armament', 'changes', 'changes', 'lash', 'la', 'laugh', 'ugh', 'open', 'often', 'often', 'often', 'someone', 'personal', 'unique', 'frantically', 'description', 'pins', 'comes', 'people', 'peril', 'purple', 'decide', 'article', 'position', 'extended', 'special', 'real', 'rely', 'rely', 'noting', 'committee', 'wanted', 'wanted', 'benefit', 'deletions', 'scissors', 'scissors', 'level', 'parallel', 'parallel', 'parallel', 'parallel', 'parallel', 'accumulation', 'consolation', 'abomination', 'plane', 'perch', 'transfixed', 'mines', 'ringing', 'accepting', 'star', 'stomachs', 'some', 'stomachs', 'unfortunately', 'considerable', 'access', 'singular', 'scarcely', 'scarcely', 'scarcely', 'scarcely', 'questionable', 'experience', 'experience', 'passable', 'represent', 'refresh', 'represent', 'represent', 'ears', 'brass', 'visitors', 'artillery', 'decided', 'benefit', 'consider', 'files', 'career', 'occurrence', 'occurrence', 'certain', 'ome', 'lie', 'establishing', 'establishing', 'different', 'ones', 'extremely', 'advisable', 'gale', 'allay', 'alley', 'alley', 'central', 'families', 'bicycle', 'bicycle', 'bicycle', 'chose', 'site', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'opposite', 'curtains', 'certain', 'countess', 'curtains', 'curtains', 'curtains', 'curious', 'dress', 'acres', 'division', 'bison', 'management', 'convenient', 'incoherent', 'convenient', 'vacant', 'supersede', 'superseded']\n",
    "\n",
    "\n",
    "a = sum(pred[i]==truth[i] for i in range(len(pred)))\n",
    "print(\"prediction score on total test set is\", a/len(pred)*100, '%')\n",
    "score=0\n",
    "total=0\n",
    "for i in range(len(truth)):\n",
    "    if truth[i] in tokens:\n",
    "        if pred[i]==truth[i]: \n",
    "            score+=1\n",
    "        total+=1\n",
    "print(\"prediction score counting only words inside our dictionary is\", score/total*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι έχει ελαφρώς καλύτερη απόδοση από τον unigram speller αφού προσφέρει περισσότερη πληροφορία στο μοντέλο, αλλά απέχει περίπου 5% από την ακρίβεια του μοντέλου που λαμβάνει υπόψη τη συχνότητα εμφάνισης των λέξεων στο λεξικό."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Μέρος 2: Χρήση σημασιολογικών αναπαραστάσεων για ανάλυση συναισθήματος\n",
    "\n",
    "### Βήμα 16\n",
    "\n",
    "Χρησιμοποιώντας τον κώδικα που μας δίνεται διαβάζουμε τα δεδομένα που κατεβάσαμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = './aclImdb/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "pos_train_dir = os.path.join(train_dir, 'pos')\n",
    "neg_train_dir = os.path.join(train_dir, 'neg')\n",
    "pos_test_dir = os.path.join(test_dir, 'pos')\n",
    "neg_test_dir = os.path.join(test_dir, 'neg')\n",
    "\n",
    "# For memory limitations. These parameters fit in 8GB of RAM.\n",
    "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
    "MAX_NUM_SAMPLES = 5000\n",
    "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
    "# sorted from most frequent to least frequent.\n",
    "# It may yield much worse results for other embeddings corpora\n",
    "NUM_W2V_TO_LOAD = 1000000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Fix numpy random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    import glob2 as glob\n",
    "except ImportError:\n",
    "    import glob\n",
    "\n",
    "import re\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub('\\s+',' ', strip_punctuation(s).lower())\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "def preproc_tok(s):\n",
    "    return tokenize(preprocess(s))\n",
    "\n",
    "def read_samples(folder, preprocess=lambda x: x):\n",
    "    samples = glob.iglob(os.path.join(folder, '*.txt'))\n",
    "    data = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
    "            break\n",
    "        with open(sample, 'r', encoding=\"utf8\") as fd:\n",
    "            x = [preprocess(l) for l in fd][0]\n",
    "            data.append(x)\n",
    "    return data\n",
    "\n",
    "def create_corpus(pos, neg):\n",
    "    corpus = np.array(pos + neg)\n",
    "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return list(corpus[indices]), list(y[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain=read_samples(neg_train_dir, preproc_tok)\n",
    "ntest=read_samples(neg_test_dir, preproc_tok)\n",
    "ptrain=read_samples(pos_train_dir,preproc_tok)\n",
    "ptest=read_samples(pos_test_dir,preproc_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 17\n",
    "\n",
    "α) Η μετρική tf-idf μας δείχνει πόσο σημαντική είναι μία λέξη για ένα corpus. Ουσιαστικά, όσο μεγαλύτερη συχνότητα εμφάνισης έχει μια λέξη στο corpus, τόσο μεγαλύτερο βάρος θα έχει στο bag of words.\n",
    "\n",
    "β) Μετράμε ουσιαστικά πόσες φορές υπάρχει μια λέξη του λεξιλογίου σε ένα σχόλιο δημιουργώντας ένα vector για κάθε δείγμα. Αυτό το vector θα έχει μέγεθος ίσο με το μέγεθος του λεξιλογίου. Έτσι αρχικά χωρίζουμε τα σχόλια σε ntrain (αρνητικά του train), ptrain (θετικά του train), ntest (αρνητικά του test), ptest (θετικά του test). Τα φέρνουμε σε μορφή τέτοια ώστε να αποτελούν μια λίστα από strings και τα δίνουμε όλα στον vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "ntrain = [str(sublist) for sublist in ntrain]\n",
    "\n",
    "ptrain = [str(sublist) for sublist in ptrain]\n",
    "\n",
    "ptest = [str(sublist) for sublist in ptest]\n",
    "\n",
    "ntest = [str(sublist) for sublist in ntest]\n",
    "\n",
    "total = ntrain + ptrain + ntest + ptest\n",
    "X = vectorizer.fit_transform(total)\n",
    "print(len(X.toarray()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η έξοδος είναι μια λίστα από vectors, όσα είναι και τα δείγματά μας. Μετατρέπουμε τον πίνακα σε np.array()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 65876\n"
     ]
    }
   ],
   "source": [
    "X = X.toarray()\n",
    "print(len(X), len(X[0]))\n",
    "# καθε set έχει 5000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χωρίζουμε τώρα τα δεδομένα σε train και test. Στα μισά δεδομένα κάθε set βάζουμε label 0 που σημαίνει ότι είναι αρνητικά σχόλια και στα υπόλοιπα μισά βάζουμε ότι είναι θετικά με label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:10000]\n",
    "X_test = X[10000:]\n",
    "y_train = [0 if i<5000 else 1 for i in range(10000)]\n",
    "y_train = np.array(y_train)\n",
    "y_test = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 65876)\n",
      "(10000,)\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "γ) Εκπαιδεύουμε στη συνέχεια τον προβλέπτη Logistic Regression δίνοντας του ως είσοδο τα train δεδομένα και υπολογίζουμε το score του με τα test δεδομένα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "reg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8564\n"
     ]
    }
   ],
   "source": [
    "print(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "δ) Με τον ίδιο τρόπο αντί για CountVectorizer χρησιμοποιούμε τον TfidfVectorizer και επαναλαμβάνουμε τα ίδια βήματα για να βρούμε το score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total)\n",
    "X = X.toarray()\n",
    "X_train = X[0:10000]\n",
    "X_test = X[10000:]\n",
    "y_train = [0 if i<5000 else 1 for i in range(10000)]\n",
    "y_train = np.array(y_train)\n",
    "y_test = y_train\n",
    "reg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
    "print(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι πράγματι υπήρξε μια μικρή βελτίωση της τάξης του 2% για τον Tfidf Vectorizer, μια και αυτή η αναπαράσταση μας δίνει με μεγαλύτερη ακρίβεια το πόσο σημαντική είναι μια λέξη για την αξιολόγηση ενός σχολίου."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 18\n",
    "α) Έχοντας το λεξιλόγιο και όλες τις λέξεις του corpus βρίσκω το αντίστοιχο ποσοστό OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = []\n",
    "for sublist in senten:\n",
    "    for item in sublist:\n",
    "        flat.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.727217283197673 %\n"
     ]
    }
   ],
   "source": [
    "total1=0\n",
    "for i in flat:\n",
    "    if i not in voc:\n",
    "        total1+=1\n",
    "print(\"percentage of words not in the vocabulary is\", total/len(flat)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "β) Παίρνοτας την matrix μορφή των embeddings που κατασκευάσαμε στο ερώτημα 9, φτιάχνουμε για κάθε σχόλιο το αντίστοιχο vector παίρνοντας τον μέσο όρο των διανυσμάτων w2vec κάθε λέξης του σχολίου.\n",
    "Αν μια λέξη δεν υπάρχει στο vocabulary βάζουμε όλα μηδενικά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = to_embeddings_Matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    news=\"\"\n",
    "    for a in s:\n",
    "        if a==' ' or a=='\\n': news+=' '\n",
    "        elif a.isalpha(): news+=a\n",
    "    return news.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emb = np.zeros(shape=(len(total), len(X1[0][0,:])))\n",
    "count = 0\n",
    "for i in total:\n",
    "    emb = np.zeros(len(X1[0][0,:]))\n",
    "    ws = tokenize(i)\n",
    "    for j in ws:\n",
    "        if j in X1[1]:\n",
    "            emb += X1[0][X1[2][j],:]\n",
    "        else:\n",
    "            emb += np.zeros(len(X1[0][0,:]))\n",
    "    emb /= len(j)\n",
    "    all_emb[count,:] = emb\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Έχοντας τα δεδομένα σε κατάλληλη μορφή τα δίνουμε ως είσοδο στον Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "reg1 = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(all_emb[0:10000], y_train)\n",
    "print(reg1.score(all_emb[10000:], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το μοντέλο μας πετυχαίνει score 70% το οποίο είναι συγκριτικά αρκετά μικρότερο σε σχέση με τα προηγούμενα ερωτήματα. Αυτό οφείλεται στο γεγονός ότι βασιστήκαμε σε προεκπαιδευμένα embeddings τα οποία δεν είχαν μεγάλη σχέση με τα training reviews μια και πιθανόν να υπήρχε αρκετά διαφορετική χρήση λέξεων και προφανώς λέξεις που δεν περιλαμβάνονταν καν στο λεξικό.\n",
    "\n",
    "Ως αποτέλεσμα, δεν έγινε τόσο καλή αξιολόγηση των reviews και καταλήξαμε σε αυτό το 70%\n",
    "\n",
    "γ)δ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x152dde9f438>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',\n",
    "binary=True, limit=NUM_W2V_TO_LOAD)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956883430481),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864823460578918),\n",
       " ('ruler', 0.5797567367553711),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('throne', 0.5422105193138123),\n",
       " ('royal', 0.5239794254302979)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clue\n",
      "[('clues', 0.6851588487625122), ('inkling', 0.6116672158241272), ('slightest_clue', 0.5809398889541626), ('know', 0.5430039763450623), ('foggiest_idea', 0.5362423658370972), ('hint', 0.5173760652542114), ('indication', 0.5164186954498291), ('faintest_idea', 0.511157751083374), ('wonder', 0.5002449154853821), ('hints', 0.492409884929657)]\n",
      "\n",
      "success\n",
      "[('successes', 0.724018394947052), ('sucess', 0.7154811024665833), ('successful', 0.6167577505111694), ('Success', 0.6160483956336975), ('succes', 0.6056337356567383), ('resounding_success', 0.5794840455055237), ('succcess', 0.5783605575561523), ('successs', 0.5095175504684448), ('hugely_successful', 0.4963102638721466), ('accomplishment', 0.49159348011016846)]\n",
      "\n",
      "whenever\n",
      "[('wherever', 0.7060423493385315), ('Whenever', 0.6958863735198975), ('everytime', 0.6308388710021973), ('anytime', 0.6082679033279419), ('whatever', 0.5835779905319214), ('frequently', 0.5679360628128052), ('invariably', 0.5635004043579102), ('if', 0.5556199550628662), ('Wherever', 0.5554419755935669), ('always', 0.5548176169395447)]\n",
      "\n",
      "killing\n",
      "[('murdering', 0.7481088638305664), ('killed', 0.7126882076263428), ('slaying', 0.6914035677909851), ('fatally_shooting', 0.6725167632102966), ('killings', 0.6411726474761963), ('murder', 0.6349565386772156), ('abducting', 0.6293994188308716), ('murdered', 0.6185826063156128), ('kiling', 0.6158840656280518), ('murders', 0.6089603900909424)]\n",
      "\n",
      "pink\n",
      "[('purple', 0.725391149520874), ('lime_green', 0.6894721984863281), ('cornflower_blue', 0.6459867358207703), ('pale_pink', 0.6373663544654846), ('bright_yellow', 0.6369321346282959), ('blue', 0.6343377828598022), ('fuchsia', 0.6262333393096924), ('bubblegum_pink', 0.6225943565368652), ('bright_orange', 0.621692419052124), ('teal', 0.6211914420127869)]\n",
      "\n",
      "analysis\n",
      "[('analyzes', 0.7746602296829224), ('statistical_analysis', 0.6847716569900513), ('Analysis', 0.6558784246444702), ('econometric_analysis', 0.603839635848999), ('assessment', 0.5894890427589417), ('anaylsis', 0.5846796035766602), ('analytical', 0.5791622400283813), ('evaluation', 0.5724815726280212), ('analyzed', 0.5600607991218567), ('analyzing', 0.545793890953064)]\n",
      "\n",
      "books\n",
      "[('book', 0.7379177808761597), ('tomes', 0.6916561126708984), ('paperback_novels', 0.6453143358230591), ('nonfiction_books', 0.6357849836349487), ('novels', 0.6332962512969971), ('booklist', 0.627798318862915), ('textbooks', 0.6162418723106384), ('ebooks', 0.6123334765434265), ('Books', 0.6083732843399048), ('eBooks', 0.6077749133110046)]\n",
      "\n",
      "fine\n",
      "[('fines', 0.6188948154449463), ('fined', 0.5436111688613892), ('fi_ne', 0.501846432685852), ('fining', 0.4933154284954071), ('Fines', 0.4886770248413086), ('Fine', 0.47894996404647827), ('hefty_fines', 0.47083038091659546), ('fines_levied', 0.42544740438461304), ('Dri_Roads', 0.4251299202442169), ('penalties', 0.4220777750015259)]\n",
      "\n",
      "son\n",
      "[('father', 0.8933086395263672), ('nephew', 0.8531849384307861), ('daughter', 0.8468297123908997), ('grandson', 0.8408660888671875), ('brother', 0.8379532098770142), ('sons', 0.7970556020736694), ('eldest_son', 0.7948955297470093), ('uncle', 0.7870238423347473), ('younger_brother', 0.7758369445800781), ('mother', 0.7683205604553223)]\n",
      "\n",
      "stared\n",
      "[('gazed', 0.7850842475891113), ('glared', 0.7507797479629517), ('staring', 0.7419307231903076), ('peered', 0.7274810075759888), ('stare', 0.7258712649345398), ('stared_blankly', 0.7116957902908325), ('glanced', 0.6932703256607056), ('stared_intently', 0.6861531734466553), ('staring_blankly', 0.6722077131271362), ('gestured', 0.6661471724510193)]\n",
      "\n",
      "stained\n",
      "[('smeared', 0.6789004802703857), ('stain', 0.6631282567977905), ('soiled', 0.6455535888671875), ('stains', 0.6134883761405945), ('spattered', 0.5942630767822266), ('splattered', 0.5908733606338501), ('soaked', 0.5898500680923462), ('dirtied', 0.589427649974823), ('caked', 0.5826871991157532), ('staining', 0.5774314999580383)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test = ['clue']\n",
    "for i in range(10):\n",
    "    test.append(voc[random.choice(range(len(voc)))])\n",
    "\n",
    "for i in test:\n",
    "    print(i)\n",
    "    print(model.most_similar(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επειδή το δείγμα μας είναι πολύ μεγαλύτερο, έχει γίνει πολύ καλή εκπαίδευση του μοντέλου με βάση τα embeddings. Έτσι είναι φανερό ότι τα αποτελέσματα είναι άμεσα συσχετισμένα με τις λέξεις που δίνονται ως είσοδος. Για παράδειγμα στη λέξη 'son' ως αποτέλεσμα δίνονται και άλλες μορφές συγγένειας, ενώ στο χρώμα 'color' 'εχουμε ως έξοδο άλλα χρώματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ε) Όπως έβγαλα αναπαραστάσεις με βάση το αρχικό corpus βγάζω τώρα με το google reviews που κατέβασα και κάνω πάλι αξιολόγηση του συστήματος με Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(model.vocab), model.vector_size))\n",
    "    word2idx = {}\n",
    "    for i in range(len(model.vocab)):\n",
    "        embedding_matrix[i] = model[model.index2word[i]]\n",
    "        word2idx[model.index2word[i]] = i\n",
    "    return embedding_matrix, model.index2word, word2idx\n",
    "\n",
    "X2 = to_embeddings_Matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2[2].get('whynot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:30.087458\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "google_emb = np.zeros(shape=(len(total), len(X2[0][0,:])))\n",
    "count = 0\n",
    "for i in total:\n",
    "    emb = np.zeros(len(X2[0][0,:]))\n",
    "    ws = tokenize(i)\n",
    "    for j in ws:\n",
    "        if X2[2].get(j) is not None:\n",
    "            emb += X2[0][X2[2][j],:]\n",
    "        else:\n",
    "            emb += np.zeros(len(X2[0][0,:]))\n",
    "    emb /= len(i)\n",
    "    google_emb[count,:] = emb\n",
    "    count+=1\n",
    "end = datetime.datetime.now()\n",
    "total_time = end - start\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7394\n"
     ]
    }
   ],
   "source": [
    "reg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(google_emb[0:10000], y_train)\n",
    "print(reg.score(google_emb[10000:], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φυσικά πολύ καλύτερο ποσοστό από το 70% στο οποίο είχαμε εκπαιδεύσει το σύστημά μας με άλλο λεξιλόγιο.\n",
    "\n",
    "στ) Τώρα κάνουμε το ίδιο αλλά πολλαπλασιάζουμε κάθε γραμμή με το βάρος tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(total)\n",
    "names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaaaargh'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " names[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:31.159473\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "google_idf_emb = np.zeros(shape=(len(total), len(X2[0][0,:])))\n",
    "count = 0\n",
    "for i in total:\n",
    "    emb = np.zeros(len(X2[0][0,:]))\n",
    "    ws = tokenize(i)\n",
    "    mysum=0\n",
    "    for j in i:\n",
    "        if X2[2].get(j) is not None and voc.get(j) is not None:\n",
    "            emb += X[count,voc[j]]*X2[0][X2[2][j],:]\n",
    "            mysum+=X[count,voc[j]]\n",
    "        else:\n",
    "            emb += np.zeros(len(X2[0][0,:]))\n",
    "    if mysum!=0:\n",
    "        emb /= mysum\n",
    "    google_idf_emb[count,:] = emb\n",
    "    count+=1\n",
    "end = datetime.datetime.now()\n",
    "total_time = end - start\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τρέχω τώρα τον classifier με βάση τα παραπάνω δεδομένα:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "reg2 = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(google_idf_emb[0:10000], y_train)\n",
    "print(reg2.score(google_idf_emb[10000:], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 19\n",
    "\n",
    "α) Δοκιμάζουμε τους ταξινομητές knn, svm με είσοδο τα google embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(google_emb[0:10000], y_train) \n",
    "print(neigh.score(google_emb[10000:], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7002\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(google_emb[0:10000], y_train) \n",
    "print(clf.score(google_emb[10000:], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6867\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(google_emb[0:10000], y_train) \n",
    "print(clf.score(google_emb[10000:], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι ο Nearest Neighbours απέδωσε εξίσου καλά με τον Logistic Regression. Ο SVM αντίστοιχα είχε μειωμένα ποσοστά τα οποία διαφοροποιούνται ανάλογα την επιλογή του kernel.\n",
    "\n",
    "β) Παρόμοια με το word2vec δοκιμάζουμε το FastText το οποίο μπορούμε να το δηλώσουμε εύκολα ως embedding από τις έτοιμες βιβλιοθήκες του gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model_ted = FastText(senten, size=100, window=5, min_count=5, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ted.train(senten, total_examples=len(senten), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clue\n",
      "[('hoped', 0.4695552885532379), ('excellent', 0.45561397075653076), ('trademark', 0.4519026577472687), ('invariably', 0.44242244958877563), ('race', 0.4399247169494629), ('trace', 0.4272076487541199), ('cheetah', 0.42570364475250244), ('rule', 0.41493621468544006), ('leadenhall', 0.4120497703552246), ('missed', 0.40796148777008057)]\n",
      "\n",
      "drawingroom\n",
      "[('watched', 0.469998836517334), ('sittingroom', 0.4655047655105591), ('receive', 0.46206986904144287), ('paddington', 0.45110204815864563), ('play', 0.44920432567596436), ('fragments', 0.44754114747047424), ('choose', 0.4471031427383423), ('status', 0.44553062319755554), ('daring', 0.445462167263031), ('dropping', 0.44527512788772583)]\n",
      "\n",
      "medium\n",
      "[('physical', 0.6026408076286316), ('providing', 0.5887846946716309), ('format', 0.5840810537338257), ('copies', 0.5456855297088623), ('works', 0.5391372442245483), ('electronic', 0.5306658744812012), ('possessed', 0.5300577282905579), ('liability', 0.5234711170196533), ('derivative', 0.5190775394439697), ('palm', 0.5184130668640137)]\n",
      "\n",
      "purpose\n",
      "[('derivative', 0.5763785243034363), ('format', 0.49784475564956665), ('breach', 0.48782798647880554), ('hundreds', 0.4582970142364502), ('disclaimer', 0.44979190826416016), ('limited', 0.44811779260635376), ('engine', 0.44544339179992676), ('destruction', 0.44492870569229126), ('bellrope', 0.4412895739078522), ('works', 0.4408523440361023)]\n",
      "\n",
      "sluggish\n",
      "[('mechanical', 0.4912453293800354), ('minds', 0.48898404836654663), ('glowing', 0.4744449257850647), ('sunlight', 0.4587351381778717), ('bank', 0.4561525881290436), ('pitiless', 0.4547349214553833), ('crawling', 0.45429491996765137), ('creatures', 0.45037752389907837), ('relation', 0.4449918866157532), ('sneer', 0.4306201934814453)]\n",
      "\n",
      "centre\n",
      "[('hollow', 0.49294227361679077), ('circle', 0.4730594754219055), ('displayed', 0.4643421471118927), ('friday', 0.46302446722984314), ('mile', 0.4399058222770691), ('vi', 0.4359511137008667), ('leaning', 0.4255097210407257), ('palm', 0.4188428521156311), ('described', 0.41575685143470764), ('viii', 0.4090653657913208)]\n",
      "\n",
      "agreed\n",
      "[('agree', 0.640031099319458), ('agreement', 0.5197767019271851), ('comply', 0.5084951519966125), ('donate', 0.498252272605896), ('goodness', 0.4863669276237488), ('ebook', 0.4843408763408661), ('c', 0.4725690484046936), ('scorched', 0.46569716930389404), ('exclaimed', 0.46125268936157227), ('files', 0.46008431911468506)]\n",
      "\n",
      "metal\n",
      "[('tripod', 0.47442707419395447), ('burning', 0.471268892288208), ('engine', 0.4631962776184082), ('stick', 0.4323597848415375), ('guns', 0.4321231245994568), ('glittering', 0.4280647039413452), ('ironclads', 0.4280078411102295), ('smell', 0.4130198359489441), ('refused', 0.40801316499710083), ('complex', 0.4047648012638092)]\n",
      "\n",
      "distributed\n",
      "[('distribute', 0.7079280614852905), ('distribution', 0.640326201915741), ('distributing', 0.5880574584007263), ('foundations', 0.5384653806686401), ('defect', 0.5374206304550171), ('copyright', 0.5354298949241638), ('defective', 0.532140851020813), ('foundation', 0.5312947034835815), ('collection', 0.5309945940971375), ('ebooks', 0.5256874561309814)]\n",
      "\n",
      "driver\n",
      "[('member', 0.465421587228775), ('chaise', 0.46109652519226074), ('bicycle', 0.4602676331996918), ('cart', 0.45801496505737305), ('roofs', 0.4521336555480957), ('devil', 0.43839970231056213), ('steadily', 0.4369633197784424), ('lieutenant', 0.4320228397846222), ('whip', 0.42806917428970337), ('alpha', 0.42771613597869873)]\n",
      "\n",
      "paused\n",
      "[('moments', 0.5189338326454163), ('hammering', 0.4894445538520813), ('thrust', 0.481356143951416), ('whip', 0.47855687141418457), ('door', 0.4622234106063843), ('coal', 0.44931721687316895), ('pew', 0.44892793893814087), ('violent', 0.4458194375038147), ('collar', 0.4429726004600525), ('breath', 0.4412729740142822)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "voc = model_ted.wv.index2word\n",
    "\n",
    "import random\n",
    "test = ['clue']\n",
    "for i in range(10):\n",
    "    test.append(voc[random.choice(range(len(voc)))])\n",
    "\n",
    "for i in test:\n",
    "    print(i)\n",
    "    print(model_ted.most_similar(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σημασιολογικά, με μια πρώτη ματιά τα similarities είναι όσο επιτυχημένα είναι και στο word2vec.\n",
    "\n",
    "Αυτό που αξίζει να σημειώσουμε είναι η εμφάνιση λέξεων με ίδια ρίζα, όπως στα ζεύγη \"distributed\"-\"distribute\" ή \"aggreed\"-\"aggreement\". Αυτό συμβαίνει γιατί στο FastText οι λέξεις παρουσιάζονται ως ένα άθροισμα διανυσμάτων, δηλαδή n-grams. Έτσι εμφανίζονται και λέξεις με ορθογραφική και ετοιμολογική σχέση και όχι μόνο σημασιολογική.\n",
    "\n",
    "Αυτό το γεγονός φαίνεται στα παραπάνω από αποτελέσματα τύπου \"drawingroom\" -\"sittingroom\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
